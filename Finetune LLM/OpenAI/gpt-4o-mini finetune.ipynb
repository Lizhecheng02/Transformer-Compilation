{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055f4ec1",
   "metadata": {},
   "source": [
    "### 1. Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa572003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88282eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"https://scienceqa.github.io/img/scienceqa.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"derek-thomas/ScienceQA\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c78075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.DataFrame(dataset[\"train\"][:10])\n",
    "dft.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_prefixes = [chr(ord(\"A\") + i) for i in range(26)] # A-Z\n",
    "def format_options(row):\n",
    "    return \" \".join([f\"({c}) {o}\" for c, o in zip(choice_prefixes, row[\"choices\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[\"choices_formatted\"] = dft.apply(format_options, axis=1)\n",
    "print(dft[\"choices\"][0])\n",
    "print(dft[\"choices_formatted\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffed8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dct = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "dft.replace({\"answer\": map_dct}, inplace=True)\n",
    "dft.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(row):\n",
    "    ret = \"Context: \" + row[\"lecture\"] + \", \" + \"Question: \" + row[\"question\"] + \", \" + \"Options: \" + row[\"choices_formatted\"]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d29588",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[\"user_content\"] = dft.apply(combine_columns, axis=1)\n",
    "dft[\"user_content\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6218a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Role: Science Teacher Assistant\n",
    "\n",
    "Task: Respond accurately to multiple-choice science questions provided by the teacher.\n",
    "\n",
    "Instructions:\n",
    "1. For each question, you will be given up to four answer choices labeled A, B, C, or D.\n",
    "2. Carefully evaluate the question and select the most appropriate answer from the provided choices.\n",
    "\n",
    "Output: Return only the letter corresponding to the correct answer (A, B, C, or D).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31dba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(user_content, answer):  \n",
    "    return {  \n",
    "        \"messages\": [  \n",
    "            {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},  \n",
    "            {\"role\": \"user\", \"content\": user_content},  \n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9389975",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.jsonl\", \"w\") as f:  \n",
    "    for i, row in list(dft.iterrows()):  \n",
    "        question = row[\"user_content\"]  \n",
    "        answer = row[\"answer\"]  \n",
    "        example = get_example(question, answer)  \n",
    "        example_str = json.dumps(example)  \n",
    "        f.write(example_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 1 train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b35e2",
   "metadata": {},
   "source": [
    "### 2. Validate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8aeb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken \n",
    "import numpy as np\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 65536 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 65536 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb18183",
   "metadata": {},
   "source": [
    "### 3. Finetune model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99102a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aeb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.files.create(\n",
    "    file=open(\"train.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1eab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = response.id\n",
    "print(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=file_id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\"n_epochs\": 2}\n",
    ")\n",
    "\n",
    "print(\"Fine-tune job is started\")\n",
    "ft_job_id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_id, limit=10)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.fine_tuning.jobs.retrieve(ft_job_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ed174",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = response.result_files\n",
    "result_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8887fe8",
   "metadata": {},
   "source": [
    "### 4. Use the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = response.fine_tuned_model\n",
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = \"\"\"\n",
    "Context: Chemical changes and physical changes are two common ways matter can change.\n",
    "\\nIn a chemical change, the type of matter changes. The types of matter before and after \n",
    "a chemical change are always different.\\nBurning a piece of paper is a chemical change. \n",
    "When paper gets hot enough, it reacts with oxygen in the air and burns. The paper and \n",
    "oxygen change into ash and smoke.\\nIn a physical change, the type of matter stays the \n",
    "same. The types of matter before and after a physical change are always the same.\n",
    "\\nCutting a piece of paper is a physical change. The cut pieces are still made of paper.\n",
    "\\nA change of state is a type of physical change. For example, ice melting is a physical \n",
    "change. Ice and liquid water are made of the same type of matter: water., \n",
    "\n",
    "Question: Complete the sentence.\\nSewing an apron is a ()., \n",
    "\n",
    "Options: (A) chemical change (B) physical change\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed97d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=fine_tuned_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee90330",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.delete(fine_tuned_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py33",
   "language": "python",
   "name": "py33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
