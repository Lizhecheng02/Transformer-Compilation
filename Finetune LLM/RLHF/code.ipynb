{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0379b8b",
   "metadata": {},
   "source": [
    "### 1 - How to get train.parquet \n",
    "[Get the dataset from here](https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons)\n",
    "### 2 - How to get train.policy.parquet\n",
    "[Get the dataset from here](https://huggingface.co/datasets/CarperAI/openai_summarize_tldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a348a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch \n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from torch import nn\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    pipeline,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from trl import (\n",
    "    RewardTrainer, \n",
    "    SFTTrainer,\n",
    "    PPOConfig,\n",
    "    PPOTrainer,\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    create_reference_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7834bd",
   "metadata": {},
   "source": [
    "## Creating policy model for human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aec46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./supervised-summarize-checkpoint\"\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "eval_batch_size = 4\n",
    "eval_steps = 500\n",
    "max_input_length = 512\n",
    "save_steps = 1000\n",
    "num_train_epochs = 5\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./train_policy.parquet\")\n",
    "df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLDRDataset(Dataset):\n",
    "    def __init__(self, train_path, tokenizer, split, max_length):\n",
    "        dataset = pd.read_parquet(train_path)\n",
    "        self.post_list = []\n",
    "        self.labels = []\n",
    "        for sample in dataset.iterrows():\n",
    "            self.post_list.append(sample[1][\"prompt\"])\n",
    "            self.labels.append(sample[1][\"label\"])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.post_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.post_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encodings_dict = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        encodings_dict_label = self.tokenizer(\n",
    "            label,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        \n",
    "        input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
    "        attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
    "        label_ids = torch.tensor(encodings_dict_label[\"input_ids\"])\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_masks,\n",
    "            \"labels\": label_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./train_policy.parquet\"\n",
    "train_dataset = TLDRDataset(\n",
    "    train_path=data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    split=\"train\",\n",
    "    max_length=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ae6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataset:\n",
    "    print(i[\"input_ids\"], i[\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f535014",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=20,\n",
    "    max_steps=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./summarization_policy\")\n",
    "tokenizer.save_pretrained(\"./summarization_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f89b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./summarization_policy\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./summarization_policy\")\n",
    "\n",
    "text = df.iloc[2][\"prompt\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfcedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(model.generate(**tokenized_text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728781f",
   "metadata": {},
   "source": [
    "## Training the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./summarization_policy\"\n",
    "DATA_PATH = \"./train.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69208d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "df = df[:100]\n",
    "raw_dataset = datasets.Dataset.from_pandas(df)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d933994",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692eb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples):\n",
    "    kwargs = {\n",
    "        \"padding\": \"max_length\",\n",
    "        \"truncation\": True,\n",
    "        \"max_length\": 256,\n",
    "        \"return_tensors\": \"pt\"\n",
    "    }\n",
    "    \n",
    "    prompt_chosen_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n",
    "    prompt_rejected_response = examples[\"prompt\"] + \"\\n\" + examples[\"rejected\"]\n",
    "    \n",
    "    tokens_chosen = tokenizer.encode_plus(prompt_chosen_response, **kwargs)\n",
    "    tokens_rejected = tokenizer.encode_plus(prompt_rejected_response, **kwargs)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0],\n",
    "        \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
    "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0],\n",
    "        \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575959c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = raw_dataset.map(formatting_func)\n",
    "formatted_dataset = formatted_dataset.train_test_split()\n",
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4429eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./reward-model-checkpoint\",\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    logging_steps=10,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    save_total_limit=1,\n",
    "    use_cpu=True,\n",
    "    max_steps=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./reward-model\")\n",
    "tokenizer.save_pretrained(\"./reward-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./reward-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./reward-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, tokenizer, prompt, response):\n",
    "    instructions = tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        response,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**instructions)\n",
    "    logits = outputs[0]\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0439dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = df.iloc[0][\"prompt\"]\n",
    "example_chosen_response = df.iloc[0][\"chosen\"]\n",
    "example_rejected_response = df.iloc[0][\"rejected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b29284",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = get_score(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    response=example_chosen_response\n",
    ")\n",
    "\n",
    "loss2 = get_score(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    response=example_rejected_response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a74cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -nn.functional.logsigmoid(loss1 - loss2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.max(loss1, axis=-1).indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.max(loss2, axis=-1).indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb89d5",
   "metadata": {},
   "source": [
    "## Policy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./reward-model\"\n",
    "DATA_PATH = \"./train.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9965c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe_kwargs = {\n",
    "    \"top_k\": None,\n",
    "    \"function_to_apply\": \"none\"\n",
    "}\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=MODEL_PATH,\n",
    "    steps=51200,\n",
    "    learning_rate=1.41e-5,\n",
    "    remove_unused_columns=True\n",
    ")\n",
    "\n",
    "txt_in_len = 5\n",
    "txt_out_len = 32 \n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"prompt\": \"review\"})\n",
    "dataset = dataset.filter(lambda x: len(x[\"review\"]) > 512, batched=False)\n",
    "dataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"input_ids\": tokenizer.encode(\n",
    "            \" \" + x[\"chosen\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=32\n",
    "        )[0]\n",
    "    },\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"query\": tokenizer.decode(x[\"input_ids\"])}, batched=False)\n",
    "dataset = dataset[:20000]\n",
    "dataset = datasets.Dataset.from_dict(dataset)\n",
    "dataset.set_format(\"pytorch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706340e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_path = \"./reward-model\"\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(rf_model_path)\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(rf_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(rf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c92191",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    ref_model=model_ref,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_str = [\"[negative]\", \"[positive]\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ctrl_tokens = dict((s, tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "    task [negative]: reward = -logit\n",
    "    task [neutral]: reward = -2 * abs(logit) + 4\n",
    "    task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i] == \"[negative]\":\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i] == \"[positive]\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"task should be in [0, 1, 2]\")\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_logit_to_reward(torch.Tensor([4, 4]), ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"eos_token_id\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, tokenizer, responses):\n",
    "    positive_logits = []\n",
    "    for i in responses:\n",
    "        instructions = tokenizer.encode_plus(\n",
    "            i,\n",
    "            padding=\"max_length\",\n",
    "            max_length=32,\n",
    "            return_tensor=\"pt\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**instructions)\n",
    "        logits = outputs[0].mean()\n",
    "        positive_logits.append(logits)\n",
    "    return positive_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33353b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        (logs, game_data, ) = (dict(), dict(), )\n",
    "        print(ctrl_str)\n",
    "        \n",
    "        task_list = choices(ctrl_str, k=config.batch_size)\n",
    "        game_data[\"query\"] = [t + q for t, q in zip(task_list, batch[\"query\"])]\n",
    "        query_tensors = [torch.cat((ctrl_tokens[t], input_ids)) for t, input_ids in zip(task_list, batch[\"input_ids\"])]\n",
    "        \n",
    "        response_tensors = []\n",
    "        for query in query_tensors:\n",
    "            response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "            response_tensors.append(response.squeeze()[-txt_out_len:])\n",
    "        print(response_tensors)\n",
    "        game_data[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "        \n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], game_data[\"query\"])]\n",
    "        logits = get_score(model, tokenizer, texts)\n",
    "        rewards = pos_logit_to_reward(logits, task_list)\n",
    "        \n",
    "        t = time.time()\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        \n",
    "        for cs in ctrl_str:\n",
    "            key = \"env/reward_\" + cs.strip(\"[]\")\n",
    "            stats[key] = np.mean([r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs])\n",
    "        \n",
    "        ppo_trainer.log_stats(stats, game_data, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./RLHF Model\")\n",
    "tokenizer.save_pretrained(\"./RLHF Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515dd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./RLHF Model\"\n",
    "pipe = pipeline(\"text-generation\", model=MODEL_PATH, \n",
    "                tokenizer=MODEL_PATH, max_length=100, \n",
    "                num_return_sequences=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
